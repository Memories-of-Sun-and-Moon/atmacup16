{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ref\n",
    "[#1 初心者向け講座 データと課題を理解してSubmitする!](https://www.guruguru.science/competitions/22/discussions/7319eed9-c403-4565-8f59-e148ec39c3f9/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# ref: Kaggleコード遺産 https://qiita.com/kaggle_grandmaster-arai-san/items/d59b2fb7142ec7e270a5 \n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# 再現性確保!\n",
    "seed_everything(427)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# data_dir\n",
    "DATA_DIR = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用のログデータと正解ラベル\n",
    "train_log_df = pl.read_csv(DATA_DIR / \"train_log.csv\")\n",
    "train_label_df = pl.read_csv(DATA_DIR / \"train_label.csv\")\n",
    "\n",
    "# 宿のデータ\n",
    "yado_df = pl.read_csv(DATA_DIR / \"yado.csv\")\n",
    "\n",
    "# テスト期間のログデータ\n",
    "test_log_df = pl.read_csv(DATA_DIR / \"test_log.csv\")\n",
    "\n",
    "sample_submission_df = pl.read_csv(DATA_DIR / \"sample_submission.csv\")\n",
    "\n",
    "# 画像のデータ\n",
    "image_df = pl.read_parquet(DATA_DIR / \"image_embeddings.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべてのログデータはあとあと参照をするので先に作っておきます.\n",
    "whole_log_df = pl.concat([train_log_df, test_log_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 宿の出現回数を計算\n",
    "yad_count = train_log_df.group_by(\"yad_no\").agg(pl.count().alias(\"yad_count\"))\n",
    "\n",
    "\n",
    "# 宿のマスター情報と紐づけて\n",
    "_df = yado_df\n",
    "_df = _df.join(yad_count, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "# left join した際の null を 0 に置き換え\n",
    "_df = _df.with_columns(\n",
    "    pl.when(\n",
    "        pl.col(\"yad_count\").is_null()\n",
    "    )\n",
    "    .then(0)\n",
    "    .otherwise(pl.col(\"yad_count\"))\n",
    "    .alias(\"yad_count\")\n",
    ")\n",
    "\n",
    "# 出現回数が多い順に並び変え\n",
    "_df = _df.sort(\"yad_count\", descending=True)\n",
    "\n",
    "# この状態で県CDごとに上位30件を取得する\n",
    "ken_top_30 = _df.group_by(\"ken_cd\").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session_yad_df(input_df: pl.DataFrame):\n",
    "    session_last = input_df.group_by(\"session_id\").agg([\n",
    "        pl.col(\"*\").last()\n",
    "    ]).join(yado_df, on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    # 県の上位30個の宿を一番最後の宿情報 `session_last` に紐づけ\n",
    "    out_df = session_last.select(\n",
    "        [\"session_id\", \"ken_cd\"]\n",
    "    ).join(\n",
    "        ken_top_30.select(\n",
    "            [\"ken_cd\", \"yad_no\"]\n",
    "        )\n",
    "        ,on=\"ken_cd\"\n",
    "        ,how=\"left\"\n",
    "    ).select(\n",
    "        [\"session_id\", \"yad_no\"]\n",
    "    )\n",
    "\n",
    "    # 使うのはセッションと宿の関係\n",
    "    out_df = out_df.select([\"session_id\", \"yad_no\"])\n",
    "\n",
    "    # ランダムに付け加えたもの以外・同一ログに出現する宿を候補に入れる\n",
    "    out_df = pl.concat([out_df, input_df.select([\"session_id\", \"yad_no\"])])\n",
    "\n",
    "    # 重複は意味がないので消す\n",
    "    out_df = out_df.unique()\n",
    "\n",
    "    # 見た目をそろえるために session / yad の順番でソート\n",
    "    out_df = out_df.sort([\"session_id\", \"yad_no\"])\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train session yado... 2.511[s]\n",
      "test session yado... 1.431[s]\n"
     ]
    }
   ],
   "source": [
    "with Timer(prefix=\"train session yado...\"):\n",
    "    train_session_yad_df = create_session_yad_df(input_df=train_log_df)\n",
    "\n",
    "# 予測の際には session ごとに yado に対しての予約確率を出さなくてはなりませんから、同じように session - yado の組を作ります。\n",
    "# ただし学習時と同じような組で良いか? は議論が必要かもしれません. (ここに現れない宿は予測対象に絶対入らないため)\n",
    "\n",
    "with Timer(prefix=\"test session yado...\"):\n",
    "    test_session_yad_df = create_session_yad_df(input_df=test_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータへの後処理: ログの一番最後の宿の削除\n",
    "# 「ログデータの一番最後の宿は必ず正解ラベルではない」という制約条件があるので、セッションの最後の宿は取り去る処理を行っておきましょう。\n",
    "def remove_last_yad_id(session_yad_df: pl.DataFrame):\n",
    "    # セッション中の一番最後の宿の組を作成\n",
    "    last_yad_df = whole_log_df.group_by(\"session_id\").agg([pl.col(\"*\").last()])\n",
    "\n",
    "    # 最後であることがわかるようにラベル is_last を付与\n",
    "    last_yad_df = last_yad_df.with_columns(pl.lit(1).alias(\"is_last\"))\n",
    "\n",
    "    # 引数の session - yad の組み合わせとマージして\n",
    "    merged = session_yad_df.join(last_yad_df, on=[\"session_id\", \"yad_no\"], how=\"left\")\n",
    "\n",
    "    # is_last ではないデータ（is_last is null）のみに絞る\n",
    "    out_df = merged.filter(pl.col(\"is_last\").is_null()).select([\"session_id\", \"yad_no\"])\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session_yad_df = remove_last_yad_id(test_session_yad_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= 1:セッションが持つ情報 -- セッションの長さ情報 =======\n",
    "session_length = whole_log_df.group_by(\"session_id\").agg(pl.max(\"seq_no\").alias(\"session_length\"))\n",
    "\n",
    "def create_session_length_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    session_length = whole_log_df.group_by(\"session_id\").agg(pl.max(\"seq_no\").alias(\"session_length\"))\n",
    "    out_df = input_df.join(session_length, on=\"session_id\", how=\"left\").select([\"session_length\"]).drop([\"session_id\"])\n",
    "    return out_df\n",
    "\n",
    "# 2回同じ関数を使って特徴を作ったとき同一のデータができるか? をテストしておくと少し安心です\n",
    "assert create_session_length_feature(train_session_yad_df).equals(create_session_length_feature(train_session_yad_df), null_equal=True)\n",
    "\n",
    "# ======= 2:宿の情報 -- 数値系特徴 =======\n",
    "def create_yado_numaric_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    num_columns = [\n",
    "        'yad_type',\n",
    "        'total_room_cnt',\n",
    "        'wireless_lan_flg',\n",
    "        'onsen_flg',\n",
    "        'kd_stn_5min',\n",
    "        'kd_bch_5min',\n",
    "        'kd_slp_5min',\n",
    "        'kd_conv_walk_5min',\n",
    "    ]\n",
    "\n",
    "    # `yad_no`をキーとして結合\n",
    "    out_df = input_df.join(yado_df.select([\"yad_no\", *num_columns]), on=\"yad_no\", how=\"left\").drop(\"yad_no\")\n",
    "    return out_df\n",
    "\n",
    "assert create_yado_numaric_feature(train_session_yad_df).equals(create_yado_numaric_feature(train_session_yad_df))\n",
    "\n",
    "# ======= 3: 宿の情報 -- wid cd の label encoding =======\n",
    "# LabelEncoding は scikit-learn に変換ロジックが用意されていますのでそれを利用するのが便利です。\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 使い方はシンプルで, LabelEncoder を定義したあと fit_trainsform で与えられた配列を数値 Label に変換します。\n",
    "le = LabelEncoder()\n",
    "\n",
    "wid_cd_label = le.fit_transform(yado_df[\"wid_cd\"])\n",
    "\n",
    "def create_yad_wid_cd_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    source_df = pl.DataFrame({\n",
    "        \"yad_no\": yado_df[\"yad_no\"],\n",
    "        \"wid_cd_label\": wid_cd_label\n",
    "    })\n",
    "\n",
    "    out_df = input_df.join(source_df, on=\"yad_no\", how=\"left\").drop(\"yad_no\")\n",
    "    return out_df\n",
    "\n",
    "assert create_yad_wid_cd_feature(train_session_yad_df).equals(create_yad_wid_cd_feature(train_session_yad_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログデータの中に宿Noは入っているのか\n",
    "\n",
    "def create_is_in_log_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # ログデータ全体から重複を排除して`is_in_log`フラグを付与\n",
    "    _df = whole_log_df.select([\"session_id\", \"yad_no\"]).unique()\n",
    "    _df = _df.with_columns(pl.lit(1).alias(\"is_in_log\"))\n",
    "\n",
    "    # 入力された session - yad と結合し、欠損値を0で置き換え\n",
    "    out_df = input_df.join(_df, on=[\"session_id\", \"yad_no\"], how=\"left\").with_columns(\n",
    "        pl.when(\n",
    "            pl.col(\"is_in_log\").is_null()\n",
    "        )\n",
    "        .then(0)\n",
    "        .otherwise(pl.col(\"is_in_log\"))\n",
    "        .alias(\"is_in_log\")\n",
    "    )\n",
    "\n",
    "    return out_df\n",
    "\n",
    "assert create_is_in_log_feature(train_session_yad_df).equals(create_is_in_log_feature(train_session_yad_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 対象と同じ地域を見ているか\n",
    "\n",
    "def create_option_yad_and_last_yado_is_same_region_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "\n",
    "    # 0: 地域のカラム名を指定(あとでべつの列でもできるように!)\n",
    "    region_column = \"sml_cd\"\n",
    "\n",
    "    # 1: セッション宿に地域を紐づけ\n",
    "    session_yad_region_df = input_df.join(yado_df.select([\"yad_no\", region_column]), on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    # 2: ログデータを使って, セッションの一番最後のレコードに地域を紐づけ\n",
    "    last_session_yad_df = whole_log_df.group_by(\"session_id\").agg(pl.col(\"yad_no\").last().alias(\"yad_no\"))\n",
    "    last_session_yad_df = last_session_yad_df.join(yado_df.select([\"yad_no\", region_column]), on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    # 3: セッション宿のセッションに, 一番最後の宿の地域を紐づけ\n",
    "    last_yad_region = session_yad_region_df.select(\"session_id\").join(last_session_yad_df.select([\"session_id\", region_column]), on=\"session_id\", how=\"left\")\n",
    "\n",
    "    # 4: 1 と 3 の地域が一致している == 一番最後の宿の地域と候補の宿の地域が一緒!\n",
    "    idx = session_yad_region_df[region_column] == last_yad_region[region_column]\n",
    "    \n",
    "    out_df = pl.DataFrame({\"same\": idx.cast(int)}).with_columns(pl.col(\"same\").alias(f\"{region_column}_is_same\"))\n",
    "\n",
    "\n",
    "    return out_df\n",
    "\n",
    "assert create_option_yad_and_last_yado_is_same_region_feature(test_session_yad_df)\\\n",
    "    .equals(create_option_yad_and_last_yado_is_same_region_feature(test_session_yad_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像の特徴を使う\n",
    "\n",
    "emb_columns = [col for col in image_df.columns if \"emb\" in col]\n",
    "\n",
    "# 次元圧縮\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 今回は 32 次元にすることにしました\n",
    "img_svd = TruncatedSVD(n_components=32)\n",
    "\n",
    "# 使い方は簡単で, shape = (n_data, n_features,) の numpy 配列を渡せばOKです\n",
    "z = img_svd.fit_transform(image_df.select(emb_columns).to_numpy())\n",
    "\n",
    "# 今回は 32 次元を指定したので z は (n_data, 32,) 次元の配列になります\n",
    "# 512 → 32 次元に圧縮ができました!\n",
    "\n",
    "# session - yado と紐付けるときは一度代表値による集約をして yado ごとの特徴に変換します (今回は max)\n",
    "svd_img_df = pl.DataFrame(data=z)\n",
    "\n",
    "max_svd_df = svd_img_df.group_by(image_df.select([\"yad_no\"])).max()\n",
    "\n",
    "# その後 yad_no で left join!\n",
    "out_df = train_session_yad_df.join(max_svd_df, on=\"yad_no\", how=\"left\").drop(\"yad_no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yado_image_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    out_df = input_df.join(max_svd_df, on=\"yad_no\", how=\"left\").drop(\"yad_no\")\n",
    "\n",
    "    return out_df.rename({col: \"yad_img_max_\" + col for col in out_df.columns if col != \"yad_no\"})\n",
    "\n",
    "assert create_yado_image_feature(train_session_yad_df).equals(create_yado_image_feature(train_session_yad_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量のマージ\n",
    "\n",
    "def create_feature(input_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    functions = [\n",
    "        create_session_length_feature,\n",
    "        create_yado_numaric_feature,\n",
    "        create_yad_wid_cd_feature,\n",
    "        create_is_in_log_feature,\n",
    "        create_yado_image_feature,\n",
    "        create_option_yad_and_last_yado_is_same_region_feature,\n",
    "    ]\n",
    "\n",
    "    out_df = pl.DataFrame()\n",
    "    for func in functions:\n",
    "        _df = func(input_df)\n",
    "        if out_df.width == 0:  # 最初の関数の出力をそのまま使う\n",
    "            out_df = _df\n",
    "        else:  # 以降はキー列を除いて結合\n",
    "            out_df = pl.concat([out_df, _df.drop([\"session_id\", \"yad_no\"])], how='horizontal')\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train... 1.610[s]\n",
      "test... 1.006[s]\n"
     ]
    }
   ],
   "source": [
    "# 実行して train / test 用の特徴量を作ります.\n",
    "\n",
    "with Timer(prefix=\"train...\"):\n",
    "    train_feat_df = create_feature(train_session_yad_df)\n",
    "\n",
    "with Timer(prefix=\"test...\"):\n",
    "    test_feat_df = create_feature(test_session_yad_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データへの後処理\n",
    "\n",
    "# 正解ラベル train_label_df の組み合わせを付与\n",
    "_df = train_session_yad_df.clone()\n",
    "\n",
    "# 重複を削除して\n",
    "_df = _df.unique()\n",
    "\n",
    "# 正解ラベルに含まれているレコードの index を配列で取得して\n",
    "target_index = train_label_df.with_columns(pl.lit(1).alias(\"target\"))\n",
    "\n",
    "# 正解Indexに含まれている場合 1 / そうでないと 0 のラベルを作成\n",
    "_df = _df.join(target_index, on=[\"session_id\", \"yad_no\"], how=\"left\")\n",
    "_df = _df.with_columns([\n",
    "    pl.when(pl.col(\"target\").is_null()).then(0).otherwise(pl.col(\"target\")).alias(\"target\")\n",
    "])\n",
    "# 見た目を揃えるために session / yad でソートしておく\n",
    "_df = _df.sort([\"session_id\", \"yad_no\"])\n",
    "\n",
    "train_session_yad_df = _df.clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = train_feat_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_idが紛れ込むからここで消す\n",
    "\n",
    "X = train_feat_df.drop(\"yad_img_max_session_id\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_session_yad_df.select(\"target\").to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "fold = GroupKFold(n_splits=5)\n",
    "cv = fold.split(X, y, groups=train_session_yad_df.select(\"session_id\").to_numpy())\n",
    "cv = list(cv) # split の返り値は generator なので list 化して何度も iterate できるようにしておく\n",
    "\n",
    "import lightgbm as lgbm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import roc_auc_score, f1_score, mean_absolute_error, mean_squared_error, \\\n",
    "    r2_score, mean_squared_log_error, median_absolute_error, explained_variance_score, cohen_kappa_score, \\\n",
    "    average_precision_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def binary_metrics(y_true: np.ndarray,\n",
    "                   predict_probability: np.ndarray,\n",
    "                   threshold=.5) -> dict:\n",
    "    \"\"\"\n",
    "    calculate binary task metrics\n",
    "    Args:\n",
    "        y_true:\n",
    "            target. shape = (n_data,)\n",
    "        predict_probability:\n",
    "            predict value. be probability prediction for log_loss, roc_auc, etc.\n",
    "        threshold:\n",
    "            Thresholds for calculating the metrics that need to be evaluated as labels\n",
    "    Returns:\n",
    "        metrics metrics dictionary. the key is metric name, and the value is score.\n",
    "    \"\"\"\n",
    "    predict_label = np.where(predict_probability > threshold, 1, 0)\n",
    "    none_prob_functions = [\n",
    "        accuracy_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score\n",
    "    ]\n",
    "\n",
    "    prob_functions = [\n",
    "        roc_auc_score,\n",
    "        log_loss,\n",
    "        average_precision_score\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "    for f in none_prob_functions:\n",
    "        score = f(y_true, predict_label)\n",
    "        scores[str(f.__name__)] = score\n",
    "    for f in prob_functions:\n",
    "        score = f(y_true, predict_probability)\n",
    "        scores[f.__name__] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "def fit_lgbm(X, \n",
    "             y, \n",
    "             cv, \n",
    "             params: dict=None):\n",
    "    \"\"\"lightGBM を CrossValidation の枠組みで学習を行なう function\"\"\"\n",
    "\n",
    "    # パラメータがないときは、空の dict で置き換える\n",
    "    if params is None:\n",
    "        params = {}\n",
    "\n",
    "    models = []\n",
    "    n_records = len(X)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "\n",
    "    for i, (idx_train, idx_valid) in enumerate(cv): \n",
    "        print(f\"-- start fold {i}\")\n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        x_train, y_train = X[idx_train], y[idx_train]\n",
    "        x_valid, y_valid = X[idx_valid], y[idx_valid]\n",
    "\n",
    "        clf = lgbm.LGBMClassifier(**params, verbose=0)\n",
    "\n",
    "        with Timer(prefix=\"fit fold={} \".format(i)):\n",
    "\n",
    "            # cv 内で train に定義された x_train で学習する\n",
    "            clf.fit(x_train, y_train, \n",
    "                    eval_set=[(x_valid, y_valid)],  \n",
    "                    callbacks=[\n",
    "                        lgbm.early_stopping(stopping_rounds=50, verbose=True),\n",
    "                        lgbm.log_evaluation(period=50, ),\n",
    "                    ],)\n",
    "\n",
    "        # cv 内で validation data とされた x_valid で予測をして oof_pred に保存していく\n",
    "        # oof_pred は全部学習に使わなかったデータの予測結果になる → モデルの予測性能を見る指標として利用できる\n",
    "        pred_i = clf.predict_proba(x_valid)[:, 1]\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        score = binary_metrics(y_valid, pred_i)\n",
    "        print(f\" - fold{i + 1} - {score}\")\n",
    "\n",
    "    score = binary_metrics(y, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISHI: Whole Score: {score}\")\n",
    "    return oof_pred, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- start fold 0\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0496838\n",
      "[100]\tvalid_0's binary_logloss: 0.0494446\n",
      "[150]\tvalid_0's binary_logloss: 0.0493173\n",
      "[200]\tvalid_0's binary_logloss: 0.049247\n",
      "[250]\tvalid_0's binary_logloss: 0.0492173\n",
      "[300]\tvalid_0's binary_logloss: 0.0491926\n",
      "[350]\tvalid_0's binary_logloss: 0.0491839\n",
      "[400]\tvalid_0's binary_logloss: 0.0491776\n",
      "[450]\tvalid_0's binary_logloss: 0.0491813\n",
      "Early stopping, best iteration is:\n",
      "[414]\tvalid_0's binary_logloss: 0.0491734\n",
      "fit fold=0  80.106[s]\n",
      " - fold1 - {'accuracy_score': 0.9843115214683115, 'f1_score': 0.18545052128836859, 'precision_score': 0.6578512396694215, 'recall_score': 0.10793952132347956, 'roc_auc_score': 0.9322439638032329, 'log_loss': 0.04917341217231733, 'average_precision_score': 0.3904399586399856}\n",
      "-- start fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.049778\n",
      "[100]\tvalid_0's binary_logloss: 0.049521\n",
      "[150]\tvalid_0's binary_logloss: 0.0493868\n",
      "[200]\tvalid_0's binary_logloss: 0.0493054\n",
      "[250]\tvalid_0's binary_logloss: 0.049265\n",
      "[300]\tvalid_0's binary_logloss: 0.0492247\n",
      "[350]\tvalid_0's binary_logloss: 0.0491999\n",
      "[400]\tvalid_0's binary_logloss: 0.0491966\n",
      "[450]\tvalid_0's binary_logloss: 0.0491945\n",
      "Early stopping, best iteration is:\n",
      "[435]\tvalid_0's binary_logloss: 0.0491936\n",
      "fit fold=1  84.802[s]\n",
      " - fold2 - {'accuracy_score': 0.9843588918067875, 'f1_score': 0.19250593617883827, 'precision_score': 0.6548463356973995, 'recall_score': 0.11283861769298663, 'roc_auc_score': 0.9315570260937339, 'log_loss': 0.049193633516717, 'average_precision_score': 0.3947549321889091}\n",
      "-- start fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0497927\n",
      "[100]\tvalid_0's binary_logloss: 0.0495514\n",
      "[150]\tvalid_0's binary_logloss: 0.049435\n",
      "[200]\tvalid_0's binary_logloss: 0.0493769\n",
      "[250]\tvalid_0's binary_logloss: 0.0493386\n",
      "[300]\tvalid_0's binary_logloss: 0.0493126\n",
      "[350]\tvalid_0's binary_logloss: 0.0493029\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's binary_logloss: 0.049297\n",
      "fit fold=2  55.602[s]\n",
      " - fold3 - {'accuracy_score': 0.9843403822833573, 'f1_score': 0.17377408185611554, 'precision_score': 0.6880712444340286, 'recall_score': 0.09944451971277604, 'roc_auc_score': 0.9308917200389031, 'log_loss': 0.0492970112703891, 'average_precision_score': 0.391236848355319}\n",
      "-- start fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0494561\n",
      "[100]\tvalid_0's binary_logloss: 0.0492001\n",
      "[150]\tvalid_0's binary_logloss: 0.0490733\n",
      "[200]\tvalid_0's binary_logloss: 0.0490165\n",
      "[250]\tvalid_0's binary_logloss: 0.0489921\n",
      "[300]\tvalid_0's binary_logloss: 0.0489744\n",
      "[350]\tvalid_0's binary_logloss: 0.0489678\n",
      "[400]\tvalid_0's binary_logloss: 0.0489588\n",
      "Early stopping, best iteration is:\n",
      "[397]\tvalid_0's binary_logloss: 0.0489579\n",
      "fit fold=3  79.117[s]\n",
      " - fold4 - {'accuracy_score': 0.9844082505359348, 'f1_score': 0.19083658380392385, 'precision_score': 0.6724102564102564, 'recall_score': 0.11119780182502799, 'roc_auc_score': 0.9324266434602565, 'log_loss': 0.04895794725128039, 'average_precision_score': 0.39639208914895824}\n",
      "-- start fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\tvalid_0's binary_logloss: 0.0499331\n",
      "[100]\tvalid_0's binary_logloss: 0.0496898\n",
      "[150]\tvalid_0's binary_logloss: 0.049549\n",
      "[200]\tvalid_0's binary_logloss: 0.0494825\n",
      "[250]\tvalid_0's binary_logloss: 0.0494426\n",
      "[300]\tvalid_0's binary_logloss: 0.049419\n",
      "[350]\tvalid_0's binary_logloss: 0.0494167\n",
      "[400]\tvalid_0's binary_logloss: 0.0494085\n",
      "[450]\tvalid_0's binary_logloss: 0.0494103\n",
      "Early stopping, best iteration is:\n",
      "[426]\tvalid_0's binary_logloss: 0.049405\n",
      "fit fold=4  79.516[s]\n",
      " - fold5 - {'accuracy_score': 0.9843059036277033, 'f1_score': 0.18382824806020653, 'precision_score': 0.6583786042624321, 'recall_score': 0.10682804448060754, 'roc_auc_score': 0.9311809601993333, 'log_loss': 0.04940504171750814, 'average_precision_score': 0.38830392493439003}\n",
      "==================================================\n",
      "FINISHI: Whole Score: {'accuracy_score': 0.9843449901848396, 'f1_score': 0.18532507486908859, 'precision_score': 0.6657020634121792, 'recall_score': 0.10764640059679224, 'roc_auc_score': 0.9316441881064951, 'log_loss': 0.04920540849104875, 'average_precision_score': 0.3922306011845823}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    # 目的関数. これの意味で最小となるようなパラメータを探します. \n",
    "    \"objective\": \"binary\", \n",
    "\n",
    "    # 木の最大数\n",
    "    \"n_estimators\": 10000, \n",
    "\n",
    "     # 学習率. 小さいほどなめらかな決定境界が作られて性能向上に繋がる場合が多いです、\n",
    "    \"learning_rate\": .1,\n",
    "\n",
    "    # 特徴重要度計算のロジック(後述)\n",
    "    \"importance_type\": \"gain\", \n",
    "    \"random_state\": 427,\n",
    "}\n",
    "\n",
    "oof, models = fit_lgbm(X, y=y, params=params, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果を解釈する\n",
    "\n",
    "# 手元でスコアを見積る\n",
    "\n",
    "def create_top_10_yad_predict(predict, session_yad_df):\n",
    "    predict_series = pl.Series(\"predict\", predict)\n",
    "    predict_df = predict_series.to_frame()\n",
    "    _df = pl.concat([session_yad_df.select([\"session_id\", \"yad_no\"]), predict_df], how=\"horizontal\")\n",
    "\n",
    "    # セッションごとに予測確率の高い順に yad_no の配列を作成\n",
    "    _agg = _df.sort(\"predict\", descending=True).group_by(\"session_id\").agg(pl.col(\"yad_no\").map_elements)\n",
    "\n",
    "    # データフレームの形式を整える\n",
    "    out_df = _agg.map_rows(lambda x: x[:10])\n",
    "\n",
    "    return out_df\n",
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the average precision at k for a single actual value.\n",
    "\n",
    "    Parameters:\n",
    "    actual : int\n",
    "        The actual value that is to be predicted\n",
    "    predicted : list\n",
    "        A list of predicted elements (order does matter)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The average precision at k\n",
    "    \"\"\"\n",
    "    if actual in predicted[:k]:\n",
    "        return 1.0 / (predicted[:k].index(actual) + 1)\n",
    "    return 0.0\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    Computes the mean average precision at k for lists of actual values and predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    actual : list\n",
    "        A list of actual values that are to be predicted\n",
    "    predicted : list\n",
    "        A list of lists of predicted elements (order does matter in the lists)\n",
    "    k : int, optional\n",
    "        The maximum number of predicted elements\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        The mean average precision at k\n",
    "    \"\"\"\n",
    "    return sum(apk(a, p, k) for a, p in zip(actual, predicted)) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162367/3189812188.py:11: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_elements`.\n",
      "  _agg = _df.sort(\"predict\", descending=True).group_by(\"session_id\").agg(pl.col(\"yad_no\").apply(list))\n",
      "/tmp/ipykernel_162367/3189812188.py:14: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_rows`.\n",
      "  out_df = _agg.apply(lambda x: x[:10])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'set_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m oof_label_df \u001b[38;5;241m=\u001b[39m create_top_10_yad_predict(predict\u001b[38;5;241m=\u001b[39moof, session_yad_df\u001b[38;5;241m=\u001b[39mtrain_session_yad_df)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# いま作成した session_id と同じ並びで train_label を並び替え\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_label \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_label_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[oof_label_df\u001b[38;5;241m.\u001b[39mindex][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myad_no\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# MAPK (k=10) として計算\u001b[39;00m\n\u001b[1;32m      7\u001b[0m oof_score \u001b[38;5;241m=\u001b[39m mapk(actual\u001b[38;5;241m=\u001b[39mtrain_label, predicted\u001b[38;5;241m=\u001b[39moof_label_df\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist(), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'set_index'"
     ]
    }
   ],
   "source": [
    "oof_label_df = create_top_10_yad_predict(predict=oof, session_yad_df=train_session_yad_df)\n",
    "\n",
    "# いま作成した session_id と同じ並びで train_label を並び替え\n",
    "train_label = train_label_df.set_index(\"session_id\").loc[oof_label_df.index][\"yad_no\"].values\n",
    "\n",
    "# MAPK (k=10) として計算\n",
    "oof_score = mapk(actual=train_label, predicted=oof_label_df.values.tolist(), k=10)\n",
    "\n",
    "print(f\"OOF Score: {oof_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atma16env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
